{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1d9e324-8d92-4104-b101-090d1a753121",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!python -m pip install --upgrade pip\n",
    "!pip install -q -U bitsandbytes einops\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git \n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U datasets\n",
    "\n",
    "!pip install nvidia-ml-py3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ebb490b-f7e4-4d01-80a5-e1bff508f50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1+cu117'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48cb416a-2e00-4866-be4b-06d4b882b0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.32.0.dev0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a0de3f-e269-4199-b1fa-d11423ae3c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.8/dist-packages (7.352.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.8 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43c1578a-7e8e-40a6-b43b-bd336db81be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79da6aa7-b583-4a10-9bb2-06928f2cc221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ea26356-9e79-42db-904e-9091065b2448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e699ca-58c8-49ac-8319-3b9d5c54bb2e",
   "metadata": {},
   "source": [
    "## QLORA 4 Bit Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aecdd44-f00f-4dcb-aa95-81086f077d64",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:5\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Define the model name\n",
    "model_name = \"EleutherAI/gpt-j-6b\"\n",
    "\n",
    "# Load the model's tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define the configuration for the quantizer\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load the model in 4-bit\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,trust_remote_code=True, device_map={\"\":0})\n",
    "\n",
    "\n",
    "# Prepare the model for LoRa, adding trainable adapters for each layer\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configuration for LoRa\n",
    "config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"query_key_value\"], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    \n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Load the dataset\n",
    "data = load_dataset(\"Abirate/english_quotes\")\n",
    "data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)\n",
    "\n",
    "# Pad tokens to max length\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Training configuration\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        warmup_steps=2,\n",
    "        max_steps=20,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        fp16_full_eval=True,\n",
    "        output_dir=\"outputs\",\n",
    "        half_precision_backend = True,\n",
    "        # optim=\"paged_adamw_8bit\",\n",
    "    \n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "result = trainer.train()\n",
    "\n",
    "print(print_summary(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a2f7d7e-c4b6-4463-88ae-498279cee4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del trainer, model, config, quant_config, data\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92665ab8-fcdc-4d4d-9eae-148cc4952235",
   "metadata": {},
   "source": [
    "## QLORA + GRADIENT CHECKPOINTING TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97b7830d-419a-4d3d-a7db-0b1259034309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883381b2a98348b298b7e07a68757115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4/20 00:03 < 00:29, 0.54 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.618900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.120500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the model name\n",
    "model_name = \"tiiuae/falcon-7b\"\n",
    "\n",
    "# Load the model's tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define the configuration for the quantizer\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load the model in 4-bit\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,trust_remote_code=True, quantization_config=quant_config, device_map={\"\":0})\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Prepare the model for LoRa, adding trainable adapters for each layer\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configuration for LoRa\n",
    "config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"query_key_value\"], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    \n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Load the dataset\n",
    "data = load_dataset(\"Abirate/english_quotes\")\n",
    "data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)\n",
    "\n",
    "# Pad tokens to max length\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Training configuration\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        warmup_steps=2,\n",
    "        max_steps=20,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        fp16_full_eval=True,\n",
    "        output_dir=\"outputs\",\n",
    "        half_precision_backend = True,\n",
    "        # optim=\"paged_adamw_8bit\",\n",
    "    \n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "result = trainer.train()\n",
    "\n",
    "print(print_summary(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f50fc7-568c-4d1a-8807-f64cb90a495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del trainer, model, config, quant_config, data\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812cbd0f-a187-4921-804e-a8f51a7d7beb",
   "metadata": {},
   "source": [
    "## Without Quantize + Gradient Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e3cb90-fafb-4334-933d-0a94212da846",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the model name\n",
    "model_name = \"tiiuae/falcon-7b\"\n",
    "\n",
    "# Load the model's tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Load the model in 4-bit\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,trust_remote_code=True, device_map={\"\":0})\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Prepare the model for LoRa, adding trainable adapters for each layer\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configuration for LoRa\n",
    "config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"query_key_value\"], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    \n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Load the dataset\n",
    "data = load_dataset(\"Abirate/english_quotes\")\n",
    "data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)\n",
    "\n",
    "# Pad tokens to max length\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Training configuration\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        warmup_steps=2,\n",
    "        max_steps=20,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        fp16_full_eval=True,\n",
    "        output_dir=\"outputs\",\n",
    "        half_precision_backend = True,\n",
    "        # optim=\"paged_adamw_8bit\",\n",
    "    \n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "result = trainer.train()\n",
    "\n",
    "print(print_summary(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58046885-c283-4f19-89e6-cbb3d916d331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b294328-629d-44a5-bafd-9fd3d4984811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e229dc0-774a-48df-aa65-8e21c5364b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1099c775-f17e-4a59-a7fd-43b23e3ea9af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c2383-dd5d-495d-83f2-800b0fa83496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10fee00-76f3-4f5a-a188-75d6e62db1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
